
## TODO
# Need to uhh reduce the image size EHHH ehh
# Get horizon cropping to work


#  Interactive
# http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_grabcut/py_grabcut.html#grabcut

# Contours
# http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_contours/py_contours_begin/py_contours_begin.html#contours-getting-started

# bg_subtraction
# https://docs.opencv.org/3.3.0/db/d5c/tutorial_py_bg_subtraction.html


# Contours
# The biggest contour *should* be the sky. This is a good heuristic. If bc of the angle the sky is divided into 2,
# then the biggest contour should still be the sky. Water might sometimes be larger, in which case we can distinguish
# from sky by position in the image

# It might help to do a noise reduction
# It might help to do a sharpening after the binarization & blur

## Surf
# So ok maybe orientation not necessary, but at the beginning let's give it a shot
http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html
- make sure surf.extended = True to get all 128 features
-

# ORB, just because it's being used

############
MAYBE find some features, find the moment of the different skyscrapers, make the crop there

#SVM
AVE FSCORE:0.3940800874772509
Params:
PREPROCESS_QUEUE = [chop_lower, grayscale, resize]
CHANNELS = cv2.IMREAD_COLOR
VOCAB_SIZE = 100
RANDOM = True
# FEATURES= 3000
FEATURES= 100

# NN
AVE FSCORE:0.3627690593848029

For CHI2 kernel and linear kernel, no matter what values of gamma and C i used, i couldn't get to
print out anything but class 3


AVE FSCORE:0.3847359310387199

one round
0.507006631713

['3', '0', '3', '3', '2', '0', '3', '0', '3', '0', '0', '0', '2', '2', '0', '0', '0', '0', '3', '0', '0', '0', '2', '1', '1', '0', '0', '3', '0', '0', '0', '2', '2', '3', '0', '0', '1', '0', '0', '2', '3', '0', '0', '3', '0', '0', '1', '0', '0', '2', '3', '2', '3', '0', '3', '1', '3', '0', '0', '2', '0', '3', '0', '2', '1', '0', '0', '2', '0', '1', '0', '3', '3', '0', '0', '3', '2', '3', '3', '0', '0', '1', '3', '0', '1', '0', '0', '0', '0', '2', '3', '0', '2', '3', '3', '3', '3', '0', '0', '0', '0', '2', '2', '0', '3', '3', '1', '2', '0', '0', '0', '0', '0', '2', '3', '0', '3', '3', '0', '0', '0', '3', '3', '0', '3', '0', '0', '0', '3', '0', '0', '2', '0', '0', '0', '0', '1', '3', '0', '0', '3', '0', '2', '3', '0', '0', '0', '3', '3', '0', '0', '3', '3', '0', '2', '2', '0', '0', '3', '2', '3', '0', '3', '1', '3', '3', '3', '2', '0', '2', '3', '0', '0', '1', '2', '0', '2', '3', '3', '0', '3', '3', '0', '3', '3', '0', '2', '0', '3', '3', '0', '2', '0', '0', '3', '0', '3', '2', '3', '3', '2', '0', '3', '1', '3', '1', '0', '2', '0', '0', '2', '0', '3', '0', '2', '3', '3', '0', '3', '3', '2', '0', '2', '3', '3', '0', '2', '2', '3', '3', '0', '3', '3', '2', '3', '0', '3', '2', '0', '3', '0', '1', '0', '3', '0', '3', '0', '2', '3', '0', '3', '0', '3', '2', '2', '0', '3', '0', '0', '1', '2', '0', '0', '1', '0', '0', '0', '0', '0', '3', '0', '3', '3', '3', '2', '3', '3', '0', '3', '0', '3', '3', '3', '0', '0', '3', '0', '0', '3', '0', '3', '0', '3', '1', '0', '0', '0', '3', '3', '3', '2', '0', '1', '0', '0', '0', '1', '0', '3', '1', '3', '0', '3', '0', '0', '3', '2', '0', '0', '0', '0', '3', '0', '3', '1', '0', '3', '2', '3', '0', '1', '3', '1', '3', '3', '1', '0', '0', '1', '2', '3', '3', '2', '3', '0', '0', '0', '0', '1', '3', '0', '0', '0', '3', '0', '3', '0', '3', '0', '0', '3', '3', '2', '0', '3', '2', '2', '0', '0', '2', '3', '1', '0', '0', '3', '0', '0', '2', '3', '0', '0', '1', '0', '0', '2', '1', '0', '2', '2', '2', '3', '0', '3', '2', '0', '0', '0', '3', '0', '0', '2', '0', '2', '0', '0', '2', '3', '2', '3', '0', '0', '0', '3', '3', '0', '3', '3', '0', '2', '3', '0', '3', '0', '2', '0', '3', '3', '2', '0', '2', '0', '1', '2', '0', '3', '3', '0', '0', '0', '3', '2', '0', '3', '2', '3', '2', '3', '3', '3', '2', '2', '0', '1', '0', '2', '0', '3', '0', '0', '0', '3', '3', '3', '0', '0', '0', '0', '3', '2', '0', '0', '0', '3', '3', '3', '0', '3', '2', '3', '0', '3', '0', '0', '2', '0', '2', '0', '0', '2', '3', '3', '3', '0', '3', '3', '2', '0', '3', '0', '3', '3', '0', '3', '0', '2', '2', '3', '3', '0', '0', '2', '0', '3', '2', '3', '2', '0', '3', '2', '3', '3', '3', '0', '2', '3', '0', '3', '0', '3', '0', '0', '3', '0', '0', '0', '3', '0', '3', '3', '0', '0', '0', '3', '3', '3', '3', '0', '3', '3', '3', '0', '2', '1', '2', '2', '3', '0', '0', '3', '0', '0', '2', '0', '0', '2', '0', '0', '0', '0', '1', '3', '0', '3', '3', '0', '0', '3', '0', '2', '2', '3', '3', '3', '3', '2', '3', '0', '3', '0', '2', '3', '1', '1', '3', '3', '0', '3', '3', '3']

"['2', '3', '3', '2', '3', '3', '3', '0', '0', '3', '0', '0', '1', '1', '3', '1', '1', '0', '1', '0', '2', '3', '0', '0', '0', '3', '0', '3', '2', '1', '1', '3', '3', '0', '0', '0', '3', '0', '0', '3', '2', '3', '0', '0', '0', '0', '0', '1', '3', '1', '0', '2', '0', '0', '0', '2', '2', '0', '0', '2', '3', '0', '3', '0', '3', '0', '0', '0', '3', '0', '3', '2', '3', '0', '3', '0', '3', '0', '3', '0', '3', '0', '0', '3', '0', '1', '2', '3', '0', '1', '3', '3', '2', '2', '0', '3', '2', '0', '3', '0', '3', '2', '3', '3', '0', '3', '0', '1', '0', '3', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '2', '3', '3', '0', '0', '0', '0', '0', '0', '3', '3', '0', '3', '0', '0', '3', '3', '3', '2', '1', '3', '2', '0', '0', '3', '0', '3', '3', '0', '0']")
"['1', '3', '1', '2', '1', '0', '2', '2', '3', '3', '0', '2', '1', '3', '3', '1', '1', '0', '1', '3', '2', '2', '2', '2', '0', '3', '1', '3', '2', '2', '0', '3', '3', '3', '0', '3', '0', '1', '1', '3', '2', '2', '1', '1', '0', '0', '3', '1', '3', '3', '0', '2', '2', '0', '0', '2', '2', '0', '0', '2', '3', '3', '3', '0', '3', '3', '2', '2', '3', '1', '3', '0', '1', '3', '2', '1', '3', '3', '3', '0', '3', '2', '3', '0', '2', '3', '2', '3', '0', '3', '3', '1', '2', '1', '0', '1', '3', '0', '0', '3', '3', '2', '3', '3', '0', '3', '2', '1', '0', '3', '0', '3', '1', '3', '0', '1', '3', '2', '0', '0', '2', '3', '1', '1', '1', '0', '0', '1', '0', '3', '3', '3', '3', '0', '0', '3', '1', '3', '3', '3', '3', '2', '0', '2', '2', '0', '3', '3', '0', '2']")
0.533451424998

3 was like 48

# JUNK
# -----------------

    # split = random.randint(0, 9)
    # print('FEATURES: {}\nVOCAB: {}\nSPLIT: {}'.format(FEATURES, VOCAB_SIZE, split))
    #
    # bov = BOV('./lists/splits/{}_train.txt'.format(split), './lists/splits/{}_test.txt'.format(split))
    # bov.train(classifier='svm')
    # res = bov.test(bov.predict)
    # # util.res_print(res)
    #
    # print('F1 Score: {}'.format(fscore(res)))
    # print(class_report(res))

    # bov.train(classifier='svm')
    # res = bov.test(bov.predict)
    # util.res_print(res)
    #
    # print('F1 Score: {}'.format(fscore(res)))
    # print(class_report(res))


    # print(accuracy(res))
    # bov.view_pics()

    # def get_hist(self, img):
    #     # Get keypoints and descriptors
    #     kp = self.ORB.detect(img, None)
    #     _, des = self.ORB.compute(img, kp)
    #
    #     # Compute the visual document, or which words are in the picture
    #     visual_doc = self.kMeans.predict(des)
    #
    #     # Construct a histogram from the visual document
    #     hist = np.zeros([VOCAB_SIZE])
    #     for w in visual_doc:
    #         hist[w] += 1
    #     return hist

    # def nn_predict(self, img):
    #     hist = self.get_hist(img)
    #     res, result, neighbors = self.knn.find_nearest(hist, 3)
    #     return int(res[0][0])

    # def predict(self, img):
    #     hist = self.get_hist(img)
    #
    #     # Use the classifier to predict the class
    #     return self.svm.predict(hist.reshape([1,-1]))

    # def test(self, func):
    #     print('Testing...')
    #     res = []
    #     for img, label in self.teststream:
    #         # In pred, label form
    #         res.append((self.predict(img), label))
    #     return res

    # def train(self, classifier='knn'):
    #     features, labels = self.get_features(self.trainstream)
    #     self.hist = self.build_hist(features, cluster=True)
    #     # self.standardize()
    #
    #     print('Training classifier...')
    #     if classifier == 'svm':
    #         self.svm.fit(self.hist, labels)
    #     else:
    #         self.knn.train(self.hist, cv2.ml.ROW_SAMPLE, labels)




FINAL TEST SHIT
sift

               precision    recall  f1-score   support

     Shanghai       0.69      0.40      0.51        63
       London       0.33      0.06      0.10        34
New York City       0.26      0.45      0.33        38
      Chicago       0.39      0.65      0.49        40

  avg / total       0.46      0.40      0.38       175

orb
0.325015662079
               precision    recall  f1-score   support

     Shanghai       0.41      0.98      0.58        63
       London       0.33      0.06      0.10        34
New York City       0.00      0.00      0.00        38
      Chicago       0.75      0.30      0.43        40

  avg / total       0.38      0.43      0.33       175


